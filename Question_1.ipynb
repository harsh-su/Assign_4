{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup and Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths\n",
    "base_dir = 'dataset'\n",
    "train_dir = 'dataset/train'\n",
    "test_dir = 'dataset/test'\n",
    "\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for folder in ['train', 'test']:\n",
    "    for category in ['dog', 'rabbit']:\n",
    "        path = os.path.join(base_dir, folder, category)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Organize files into train and test folders\n",
    "for category in ['dog', 'rabbit']:\n",
    "    category_path = os.path.join(base_dir, category)\n",
    "    images = os.listdir(category_path)\n",
    "    train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Move and reshape images to the train and test folders\n",
    "    for img in train_images:\n",
    "        img_path = os.path.join(category_path, img)\n",
    "        # Read and resize image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is not None:\n",
    "            resized_image = cv2.resize(image, (40, 40))\n",
    "            cv2.imwrite(os.path.join(train_dir, category, img), resized_image)\n",
    "\n",
    "    for img in test_images:\n",
    "        img_path = os.path.join(category_path, img)\n",
    "        # Read and resize image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is not None:\n",
    "            resized_image = cv2.resize(image, (40, 40))\n",
    "            cv2.imwrite(os.path.join(test_dir, category, img), resized_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, train_it, test_it, epochs=7):\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     history = model.fit(train_it, steps_per_epoch=len(train_it), validation_data=test_it, validation_steps=len(test_it), epochs=epochs, verbose=1)\n",
    "    \n",
    "#     training_time = time.time() - start_time\n",
    "#     training_loss = history.history['loss'][-1]\n",
    "#     training_accuracy = history.history['accuracy'][-1]\n",
    "    \n",
    "#     # Evaluate the model on the test set\n",
    "#     _, test_accuracy = model.evaluate(test_it, steps=len(test_it), verbose=0)\n",
    "    \n",
    "#     # Get the number of parameters in the model\n",
    "#     num_params = model.count_params()\n",
    "    \n",
    "#     return training_time, training_loss, training_accuracy, test_accuracy, num_params\n",
    "\n",
    "def train_model_with_tensorboard(model, train_it, test_it, epochs=7):\n",
    "    # Create a TensorBoard callback\n",
    "    tensorboard_callback = TensorBoard(log_dir='/home/harshal/ml/log_data', histogram_freq=1)\n",
    "    \n",
    "    # Start training with the callback\n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(train_it, steps_per_epoch=len(train_it),\n",
    "                        validation_data=test_it, validation_steps=len(test_it),\n",
    "                        epochs=epochs, verbose=1,\n",
    "                        callbacks=[tensorboard_callback])  # Pass the callback here\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    training_loss = history.history['loss'][-1]\n",
    "    training_accuracy = history.history['accuracy'][-1]\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    _, test_accuracy = model.evaluate(test_it, steps=len(test_it), verbose=0)\n",
    "    \n",
    "    # Get the number of parameters in the model\n",
    "    num_params = model.count_params()\n",
    "    \n",
    "    return training_time, training_loss, training_accuracy, test_accuracy, num_params\n",
    "\n",
    "# Now train the model and log with TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 195 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data preprocessing without augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)  # Only rescaling for training data\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)   # Only rescaling for test data\n",
    "\n",
    "# Prepare iterators\n",
    "train_it = train_datagen.flow_from_directory(\n",
    "    'dataset/train/', class_mode='binary', batch_size=32, target_size=(40, 40), shuffle=True)\n",
    "\n",
    "test_it = test_datagen.flow_from_directory(\n",
    "    'dataset/test/', class_mode='binary', batch_size=32, target_size=(40, 40), shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 195\n",
      "Number of validation samples: 75\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples:\", train_it.samples)\n",
    "print(\"Number of validation samples:\", test_it.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG (1 Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_block_1(input_shape=(40, 40, 3)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    opt = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5599 - loss: 2.6705 - val_accuracy: 0.5333 - val_loss: 1.2696\n",
      "Epoch 2/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7154 - loss: 0.6515 - val_accuracy: 0.7067 - val_loss: 0.5420\n",
      "Epoch 4/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7811 - loss: 0.4453 - val_accuracy: 0.8400 - val_loss: 0.3126\n",
      "Epoch 6/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8810 - loss: 0.2400 - val_accuracy: 0.8933 - val_loss: 0.2449\n",
      "Epoch 8/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 9/20\n",
      "\u001b[1m 1/25\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.1096"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 12:46:04.374268: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9113 - loss: 0.2308 - val_accuracy: 0.9867 - val_loss: 0.1152\n",
      "Epoch 10/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 11/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9943 - loss: 0.0775 - val_accuracy: 0.9867 - val_loss: 0.0913\n",
      "Epoch 12/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 13/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0569 - val_accuracy: 0.9733 - val_loss: 0.0891\n",
      "Epoch 14/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 15/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9974 - loss: 0.0436 - val_accuracy: 0.9733 - val_loss: 0.0902\n",
      "Epoch 16/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 17/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9977 - loss: 0.0477 - val_accuracy: 0.9867 - val_loss: 0.0532\n",
      "Epoch 18/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 19/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0215 - val_accuracy: 0.9867 - val_loss: 0.0461\n",
      "Epoch 20/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model = VGG_block_1()\n",
    "training_time, training_loss, training_accuracy, test_accuracy, num_params = train_model_with_tensorboard(model, train_it, test_it, epochs=20)\n",
    "results.append([\"VGG (Block1)\", training_time, training_loss, training_accuracy, test_accuracy, num_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - accuracy: 0.3990 - loss: 2.3055 - val_accuracy: 0.6933 - val_loss: 0.5413\n",
      "Epoch 2/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6871 - loss: 0.5967 - val_accuracy: 0.8267 - val_loss: 0.4290\n",
      "Epoch 4/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/20\n",
      "\u001b[1m1/7\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7812 - loss: 0.4651"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 12:53:10.230060: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7494 - loss: 0.4788 - val_accuracy: 0.8400 - val_loss: 0.3844\n",
      "Epoch 6/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7821 - loss: 0.4620 - val_accuracy: 0.7733 - val_loss: 0.3726\n",
      "Epoch 8/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 9/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8220 - loss: 0.3674 - val_accuracy: 0.9733 - val_loss: 0.2510\n",
      "Epoch 10/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 11/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8951 - loss: 0.2902 - val_accuracy: 0.9733 - val_loss: 0.2136\n",
      "Epoch 12/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 13/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9552 - loss: 0.2205 - val_accuracy: 0.9600 - val_loss: 0.1965\n",
      "Epoch 14/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 15/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9525 - loss: 0.1934 - val_accuracy: 0.9200 - val_loss: 0.1884\n",
      "Epoch 16/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 17/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9310 - loss: 0.2025 - val_accuracy: 0.9333 - val_loss: 0.2207\n",
      "Epoch 18/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 19/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9391 - loss: 0.2015 - val_accuracy: 0.9733 - val_loss: 0.1334\n",
      "Epoch 20/20\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.427165269851685, 0.0, 0.0, 0.9733333587646484, 1639553)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGG_block_1()\n",
    "train_model_with_tensorboard(model, train_it, test_it, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG (Block 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG (3 blocks) model\n",
    "def VGG_block_3(input_shape=(40, 40, 3)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5356 - loss: 0.9127 - val_accuracy: 0.5000 - val_loss: 0.7707\n",
      "Epoch 2/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4926 - loss: 0.7289 - val_accuracy: 0.5750 - val_loss: 0.6207\n",
      "Epoch 4/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6742 - loss: 0.5910 - val_accuracy: 0.7500 - val_loss: 0.5649\n",
      "Epoch 6/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 925us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7792 - loss: 0.5588 - val_accuracy: 0.7750 - val_loss: 0.5460\n"
     ]
    }
   ],
   "source": [
    "model = VGG_block_3()\n",
    "training_time, training_loss, training_accuracy, test_accuracy, num_params = evaluate_model(model, train_it, test_it, epochs=7)\n",
    "results.append([\"VGG (Block 3)\", training_time, training_loss, training_accuracy, test_accuracy, num_params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG (Block 3) with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_block_3_augmented(input_shape=(40, 40, 3)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160 images belonging to 2 classes.\n",
      "Found 40 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 4. Prepare dataset generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_it_aug = train_datagen.flow_from_directory(\n",
    "    'dataset/train/',\n",
    "    class_mode='binary', batch_size=64, target_size=(40, 40))\n",
    "\n",
    "test_it_aug = test_datagen.flow_from_directory(\n",
    "    'dataset/test/',\n",
    "    class_mode='binary', batch_size=64, target_size=(40, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.5451 - loss: 1.1482 - val_accuracy: 0.5000 - val_loss: 0.8222\n",
      "Epoch 2/7\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/7\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4902 - loss: 0.8784 - val_accuracy: 0.5000 - val_loss: 0.8267\n",
      "Epoch 4/7\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/7\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5367 - loss: 0.7287 - val_accuracy: 0.5000 - val_loss: 0.7413\n",
      "Epoch 6/7\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/7\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.4922 - loss: 0.7436 - val_accuracy: 0.5000 - val_loss: 0.7498\n"
     ]
    }
   ],
   "source": [
    "model = VGG_block_3_augmented()\n",
    "training_time, training_loss, training_accuracy, test_accuracy, num_params = evaluate_model(model, train_it_aug, test_it_aug, epochs=7)\n",
    "results.append([\"VGG (3 Blocks) with Augmentation\", training_time, training_loss, training_accuracy, test_accuracy, num_params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning using VGG16 or VGG19 with tuning all layers (including tuning convolution layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning with VGG16 (all layers)\n",
    "def VGG16_All_Layer_Tuning():\n",
    "    model = VGG16(include_top=False, input_shape=(40, 40, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    flat1 = Flatten()(model.layers[-1].output)\n",
    "    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "    output = Dense(1, activation='sigmoid')(class1)\n",
    "    \n",
    "    model = Model(inputs=model.inputs, outputs=output)\n",
    "    \n",
    "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_137']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 237ms/step - accuracy: 0.6253 - loss: 0.6746 - val_accuracy: 0.8500 - val_loss: 0.4961\n",
      "Epoch 2/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - accuracy: 0.8084 - loss: 0.4309 - val_accuracy: 0.8250 - val_loss: 0.3116\n",
      "Epoch 4/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 213ms/step - accuracy: 0.8983 - loss: 0.2312 - val_accuracy: 0.9500 - val_loss: 0.1852\n",
      "Epoch 6/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 215ms/step - accuracy: 0.9730 - loss: 0.0914 - val_accuracy: 0.9250 - val_loss: 0.2531\n"
     ]
    }
   ],
   "source": [
    "model = VGG16_All_Layer_Tuning()\n",
    "training_time, training_loss, training_accuracy, test_accuracy, num_params = evaluate_model(model, train_it, test_it, epochs=7)\n",
    "results.append([\"Transfer Learning VGG16 (All Layers)\", training_time, training_loss, training_accuracy, test_accuracy, num_params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transfer learning using VGG16  with tuning only final MLP layers (excluding convolution layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning with VGG16 (MLP only)\n",
    "def VGG16_MLP_Tuning():\n",
    "    model = VGG16(include_top=False, input_shape=(40, 40, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    flat1 = Flatten()(model.layers[-1].output)\n",
    "    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
    "    output = Dense(1, activation='sigmoid')(class1)\n",
    "    \n",
    "    model = Model(inputs=model.inputs, outputs=output)\n",
    "    \n",
    "    opt = SGD(learning_rate=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_159']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.5139 - loss: 0.7913 - val_accuracy: 0.5000 - val_loss: 0.7022\n",
      "Epoch 2/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.4595 - loss: 0.7021 - val_accuracy: 0.5250 - val_loss: 0.6944\n",
      "Epoch 4/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.5097 - loss: 0.6913 - val_accuracy: 0.5500 - val_loss: 0.6649\n",
      "Epoch 6/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0s/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.6096 - loss: 0.6619 - val_accuracy: 0.6750 - val_loss: 0.6510\n"
     ]
    }
   ],
   "source": [
    "model = VGG16_MLP_Tuning()\n",
    "training_time, training_loss, training_accuracy, test_accuracy, num_params = evaluate_model(model, train_it, test_it, epochs=7)\n",
    "results.append([\"Transfer Learning VGG16 (MLP only)\", training_time, training_loss, training_accuracy, test_accuracy, num_params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate The Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Time (s)</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Training Accuracy (%)</th>\n",
       "      <th>Testing Accuracy (%)</th>\n",
       "      <th>Number of Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VGG (Block1)</td>\n",
       "      <td>2.024451</td>\n",
       "      <td>0.556424</td>\n",
       "      <td>0.68125</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1639553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VGG (Block 3)</td>\n",
       "      <td>1.468822</td>\n",
       "      <td>0.540392</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.775</td>\n",
       "      <td>503105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VGG (3 Blocks) with Augmentation</td>\n",
       "      <td>1.653190</td>\n",
       "      <td>0.746057</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>503105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transfer Learning VGG16 (All Layers)</td>\n",
       "      <td>10.245744</td>\n",
       "      <td>0.160877</td>\n",
       "      <td>0.93750</td>\n",
       "      <td>0.925</td>\n",
       "      <td>14780481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transfer Learning VGG16 (MLP only)</td>\n",
       "      <td>3.831149</td>\n",
       "      <td>0.659223</td>\n",
       "      <td>0.63125</td>\n",
       "      <td>0.675</td>\n",
       "      <td>14780481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Model  Training Time (s)  Training Loss  \\\n",
       "0                          VGG (Block1)           2.024451       0.556424   \n",
       "1                         VGG (Block 3)           1.468822       0.540392   \n",
       "2      VGG (3 Blocks) with Augmentation           1.653190       0.746057   \n",
       "3  Transfer Learning VGG16 (All Layers)          10.245744       0.160877   \n",
       "4    Transfer Learning VGG16 (MLP only)           3.831149       0.659223   \n",
       "\n",
       "   Training Accuracy (%)  Testing Accuracy (%)  Number of Parameters  \n",
       "0                0.68125                 0.700               1639553  \n",
       "1                0.78125                 0.775                503105  \n",
       "2                0.50000                 0.500                503105  \n",
       "3                0.93750                 0.925              14780481  \n",
       "4                0.63125                 0.675              14780481  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Display results\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Training Time (s)\", \"Training Loss\", \"Training Accuracy (%)\", \"Testing Accuracy (%)\", \"Number of Parameters\"])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an MLP model with parameters comparable to VGG16 and compare your performance with the other models in the table. You can choose the distribution of the number of neurons and the number of layers. What can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "def MLP_Classifier(input_shape=(40, 40, 3)):\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=input_shape),                 # Flatten the input images\n",
    "        Dense(1024, activation='relu'),                   # Dense layer 1\n",
    "        Dropout(0.5),                                     # Dropout layer for regularization\n",
    "        Dense(2048, activation='relu'),                   # Dense layer 2\n",
    "        Dropout(0.5),\n",
    "        Dense(2048, activation='relu'),                   # Dense layer 3\n",
    "        Dropout(0.5),\n",
    "        Dense(1024, activation='relu'),                   # Dense layer 4\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='relu'),                    # Dense layer 5\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')                    # Output layer for binary classification\n",
    "    ])\n",
    "    \n",
    "    # Optimizer\n",
    "    opt = SGD(learning_rate=0.00001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.4720 - loss: 0.8335 - val_accuracy: 0.5000 - val_loss: 0.7033\n",
      "Epoch 2/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 3/7\n",
      "\u001b[1m 2/10\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4688 - loss: 0.9364"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.4853 - loss: 0.8530 - val_accuracy: 0.5000 - val_loss: 0.7028\n",
      "Epoch 4/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 5/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.4470 - loss: 0.8511 - val_accuracy: 0.5000 - val_loss: 0.7021\n",
      "Epoch 6/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00\n",
      "Epoch 7/7\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - accuracy: 0.4877 - loss: 0.8250 - val_accuracy: 0.5000 - val_loss: 0.7016\n"
     ]
    }
   ],
   "source": [
    "model = MLP_Classifier()\n",
    "training_time, training_loss, training_accuracy, test_accuracy, num_params = evaluate_model(model, train_it, test_it, epochs=7)\n",
    "results.append([\"MLP \", training_time, training_loss, training_accuracy, test_accuracy, num_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Time (s)</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Training Accuracy (%)</th>\n",
       "      <th>Testing Accuracy (%)</th>\n",
       "      <th>Number of Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VGG (Block1)</td>\n",
       "      <td>2.024451</td>\n",
       "      <td>0.556424</td>\n",
       "      <td>0.68125</td>\n",
       "      <td>0.700</td>\n",
       "      <td>1639553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VGG (Block 3)</td>\n",
       "      <td>1.468822</td>\n",
       "      <td>0.540392</td>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.775</td>\n",
       "      <td>503105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VGG (3 Blocks) with Augmentation</td>\n",
       "      <td>1.653190</td>\n",
       "      <td>0.746057</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>503105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transfer Learning VGG16 (All Layers)</td>\n",
       "      <td>10.245744</td>\n",
       "      <td>0.160877</td>\n",
       "      <td>0.93750</td>\n",
       "      <td>0.925</td>\n",
       "      <td>14780481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transfer Learning VGG16 (MLP only)</td>\n",
       "      <td>3.831149</td>\n",
       "      <td>0.659223</td>\n",
       "      <td>0.63125</td>\n",
       "      <td>0.675</td>\n",
       "      <td>14780481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>VGG (Block1)</td>\n",
       "      <td>10.343411</td>\n",
       "      <td>0.717865</td>\n",
       "      <td>0.58750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>47460353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP</td>\n",
       "      <td>8.001243</td>\n",
       "      <td>0.756027</td>\n",
       "      <td>0.56250</td>\n",
       "      <td>0.700</td>\n",
       "      <td>34875393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP</td>\n",
       "      <td>6.296185</td>\n",
       "      <td>0.741144</td>\n",
       "      <td>0.51250</td>\n",
       "      <td>0.500</td>\n",
       "      <td>25436673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLP</td>\n",
       "      <td>3.044645</td>\n",
       "      <td>0.837747</td>\n",
       "      <td>0.43750</td>\n",
       "      <td>0.500</td>\n",
       "      <td>7706113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP</td>\n",
       "      <td>3.401530</td>\n",
       "      <td>0.813059</td>\n",
       "      <td>0.50625</td>\n",
       "      <td>0.475</td>\n",
       "      <td>10688513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP</td>\n",
       "      <td>4.142219</td>\n",
       "      <td>0.808165</td>\n",
       "      <td>0.50625</td>\n",
       "      <td>0.500</td>\n",
       "      <td>13835265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Model  Training Time (s)  Training Loss  \\\n",
       "0                           VGG (Block1)           2.024451       0.556424   \n",
       "1                          VGG (Block 3)           1.468822       0.540392   \n",
       "2       VGG (3 Blocks) with Augmentation           1.653190       0.746057   \n",
       "3   Transfer Learning VGG16 (All Layers)          10.245744       0.160877   \n",
       "4     Transfer Learning VGG16 (MLP only)           3.831149       0.659223   \n",
       "5                           VGG (Block1)          10.343411       0.717865   \n",
       "6                                   MLP            8.001243       0.756027   \n",
       "7                                   MLP            6.296185       0.741144   \n",
       "8                                   MLP            3.044645       0.837747   \n",
       "9                                   MLP            3.401530       0.813059   \n",
       "10                                  MLP            4.142219       0.808165   \n",
       "\n",
       "    Training Accuracy (%)  Testing Accuracy (%)  Number of Parameters  \n",
       "0                 0.68125                 0.700               1639553  \n",
       "1                 0.78125                 0.775                503105  \n",
       "2                 0.50000                 0.500                503105  \n",
       "3                 0.93750                 0.925              14780481  \n",
       "4                 0.63125                 0.675              14780481  \n",
       "5                 0.58750                 0.750              47460353  \n",
       "6                 0.56250                 0.700              34875393  \n",
       "7                 0.51250                 0.500              25436673  \n",
       "8                 0.43750                 0.500               7706113  \n",
       "9                 0.50625                 0.475              10688513  \n",
       "10                0.50625                 0.500              13835265  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Display results\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Training Time (s)\", \"Training Loss\", \"Training Accuracy (%)\", \"Testing Accuracy (%)\", \"Number of Parameters\"])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters in MLP in VGG16 are almost equest but VGG gives much higher accuracy there are few reasons.\n",
    "-  VGG16 uses convolutional layers to capture spatial and hierarchical features in images, which MLPs cannot do effectively.\n",
    "-  CNNs like VGG16 use parameters more efficiently by focusing on local relationships, wheresd MLPs treat each pixel independently and loses structural information.\n",
    "- VGG16’s layered structure allows it to learn low- to high-level features progressively, while MLPs lack this hierarchical capability\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " use any image generation tool of your choice. Provide a prompt that helps easily classify your image; and provide a prompt that creates a hard to correctly classify image. Show the performance of various models on these sets of images. Minimum 4 such images can be created. Class A: easy, Class A: hard; Class B: easy, Class B: hard [1 mark]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Leonardo AI for image generating\n",
    "https://leonardo.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
